{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19d5d66a-f27b-4eb3-95a3-69bae9c920b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,gc,json,warnings,random,torch\n",
    "sys.path.append(\"./share\")\n",
    "sys.path.append(\"./common\")\n",
    "sys.path.append(\"./llama\")\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from LLMCommon import CLLMCommon\n",
    "from LLMDataset import CLLMDataset\n",
    "from LLMFTRegression import CLLMFTRegression\n",
    "from LLMFLClassify import CLLMFLClassify,evalulate_classify\n",
    "from LLMModelClassify import CLLMModelClassify\n",
    "from Config import g_data_root,g_embedding_shape,get_attack_score\n",
    "from IoTSample import CIoTSample\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "g_token_root = \"%stoken\"%g_data_root\n",
    "g_embedding_root = \"%sembedding\"%g_data_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03bd5a19-6274-4f55-ad8d-9003379a873d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0577411d32407ca27e45bb59550348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = CLLMModelClassify(0)\n",
    "model.load_huggingface()\n",
    "model.m_model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "642dff57-a855-4ec6-b6cb-cb47b63b8272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(df_data,max_length = 640*1024):\n",
    "    embedding = model.predict(df_data['text'].tolist(),batch_size=1,max_length=max_length)\n",
    "    noised = model.predict(df_data['noised'].tolist(),batch_size=1,max_length=max_length)\n",
    "    df_ret = df_data.copy(deep = True)\n",
    "    df_ret['token_embedding'] = pd.Series(embedding)\n",
    "    df_ret['noised_embedding'] = pd.Series(noised)\n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8d47ac2-8f7b-469a-813d-f44eacbdd4e0",
   "metadata": {},
   "source": [
    "ioTSample = CIoTSample()\n",
    "for attack in tqdm(ioTSample.get_attack_type(),desc=\"Total\"):\n",
    "    if not attack in ['DDoS ICMP Flood Attacks','Backdoor_attack','DDoS HTTP Flood Attacks']:\n",
    "        text_2_embedding(attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43e1b11f-52aa-432d-969f-564e051af2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLLMSample:\n",
    "        \n",
    "    def __init__(self,attack):\n",
    "        self.m_attack = attack\n",
    "        index_file = \"%s/%s/index.csv\"%(g_token_root,attack)\n",
    "        self.m_df_index = pd.read_csv(index_file,index_col=0)\n",
    "        \n",
    "    def read_text(self, file_name):\n",
    "        with open(file_name,'r') as fp:\n",
    "            data = fp.read()\n",
    "            text = \"<|begin_of_text|>\"+data+\"<|end_of_text|>\"\n",
    "        return text\n",
    "\n",
    "    def resize_embedding(self,embedding1):\n",
    "        embedding = np.array(embedding1).squeeze()\n",
    "        array_list = embedding.tolist()\n",
    "        embedding = np.array(array_list)\n",
    "        embedding = torch.tensor(embedding,dtype=torch.float32)\n",
    "        pooled_tensor = F.adaptive_avg_pool2d(embedding.unsqueeze(0).unsqueeze(0), g_embedding_shape)\n",
    "        pooled_tensor = pooled_tensor.squeeze().numpy().tolist()\n",
    "        return pooled_tensor\n",
    "        \n",
    "    def write_embedding(self,file_name,embedding):\n",
    "        with open(file_name,\"w\") as fp1:\n",
    "            json_str = json.dumps(embedding)\n",
    "            fp1.write(json_str)\n",
    "                \n",
    "    def get_group_text(self,group):\n",
    "        df_tmp = self.m_df_index[self.m_df_index['group' ] == group ]\n",
    "        if df_tmp['Label'].nunique() > 1 :\n",
    "            print(\"There are too much Labels in one Group\")\n",
    "            return None, None, None\n",
    "        if df_tmp['score'].nunique() > 1 :\n",
    "            print(\"There are too much score in one Group\")\n",
    "            return None, None, None\n",
    "        if df_tmp['llm_token_file'].nunique() > 1 :\n",
    "            print(\"There are too much llm_token_file in one Group\")\n",
    "            return None, None, None\n",
    "        if df_tmp['llm_noised_file'].nunique() > 1 :\n",
    "            print(\"There are too much llm_noised_file in one Group\")\n",
    "            return None, None, None\n",
    "            \n",
    "        raw_score = df_tmp.iloc[0]['score']\n",
    "        raw_labels = json.loads(df_tmp.iloc[0]['Label'])\n",
    "        score = get_attack_score(pd.Series(raw_labels))\n",
    "        llm_token_file = df_tmp.iloc[0]['llm_token_file']\n",
    "        llm_noised_file = df_tmp.iloc[0]['llm_noised_file']\n",
    "\n",
    "        def compare_floats_round(a, b, precision):\n",
    "            return round(a, precision) == round(b, precision)\n",
    "        if not compare_floats_round(raw_score, score, 7):\n",
    "            display(raw_score,score)\n",
    "            print(\"Algorithm is rereshed continue...\") \n",
    "            \n",
    "        llm_text = self.read_text(llm_token_file)\n",
    "        noised_text = self.read_text(llm_noised_file)\n",
    "        return score, llm_text, noised_text,llm_token_file,llm_noised_file\n",
    "        \n",
    "    def get_batch_text(self,group_list):\n",
    "        all_data =  []\n",
    "        for group in group_list:\n",
    "            score, llm_text, noised_text,llm_token_file,llm_noised_file = self.get_group_text(group)\n",
    "            tmp = {}\n",
    "            tmp['score'] = score\n",
    "            tmp['text'] = llm_text\n",
    "            tmp['noised'] = noised_text\n",
    "            tmp['llm_token_file'] = llm_token_file\n",
    "            tmp['llm_noised_file'] = llm_noised_file\n",
    "            file_name = llm_token_file.split(\"/\")[-1]\n",
    "            token_embedding_path = \"%s/attack/%s/\"%(g_embedding_root,self.m_attack)\n",
    "            noised_embedding_path = \"%s/noised/%s/\"%(g_embedding_root,self.m_attack)\n",
    "            os.makedirs(token_embedding_path, exist_ok=True)\n",
    "            os.makedirs(noised_embedding_path, exist_ok=True)\n",
    "            tmp['token_embedding_file'] = \"%s/G%d-%s\"%(token_embedding_path,group,file_name)\n",
    "            tmp['noised_embedding_file'] = \"%s/G%d-%s\"%(noised_embedding_path,group,file_name)\n",
    "            all_data.append(tmp)\n",
    "            \n",
    "        return pd.DataFrame(all_data)\n",
    "        \n",
    "    def get_group_batch(self,step = 100 ):\n",
    "        def split_into_ranges(a, t):\n",
    "            max_val = max(a)\n",
    "            return [[i, min(i + t, max_val + 1)] for i in range(0, max_val + 1, t)]\n",
    "        groups = self.m_df_index['group'].unique().tolist()\n",
    "        return split_into_ranges(groups,step)\n",
    "\n",
    "    def Create_Embedding(self,batch_size = 5):\n",
    "        batch_range = self.get_group_batch(batch_size)\n",
    "        count = 0\n",
    "        for r in tqdm(batch_range):\n",
    "            group_list = list(range(r[0],r[1]))\n",
    "            print(\"Embedding\",group_list)\n",
    "            df_text = self.get_batch_text(group_list)\n",
    "            df_embedding = get_embedding(df_text)\n",
    "            for i,row in df_embedding.iterrows():\n",
    "                token_embedding = self.resize_embedding(row['token_embedding'])\n",
    "                noised_embedding = self.resize_embedding(row['noised_embedding'])\n",
    "                self.write_embedding(row['token_embedding_file'],token_embedding)\n",
    "                self.write_embedding(row['noised_embedding_file'],noised_embedding)\n",
    "                count = count + 1\n",
    "            if count > 2000:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74751d6d-2701-4bb8-84f5-0a2daf05c149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d13612172641b9862a634d0c3e6a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding [0, 1, 2, 3, 4]\n",
      "Embedding [5, 6, 7, 8, 9]\n",
      "Embedding [10, 11, 12, 13, 14]\n",
      "Embedding [15, 16, 17, 18, 19]\n",
      "Embedding [20, 21, 22, 23, 24]\n",
      "Embedding [25, 26, 27, 28, 29]\n",
      "Embedding [30, 31, 32, 33, 34]\n",
      "Embedding [35, 36, 37, 38, 39]\n",
      "Embedding [40, 41, 42, 43, 44]\n",
      "Embedding [45, 46, 47, 48, 49]\n",
      "Embedding [50, 51, 52, 53, 54]\n",
      "Embedding [55, 56, 57, 58, 59]\n",
      "Embedding [60, 61, 62, 63, 64]\n",
      "Embedding [65, 66, 67, 68, 69]\n",
      "Embedding [70, 71, 72, 73, 74]\n",
      "Embedding [75, 76, 77, 78, 79]\n",
      "Embedding [80, 81, 82, 83, 84]\n",
      "Embedding [85, 86, 87, 88, 89]\n"
     ]
    }
   ],
   "source": [
    "attack = 'Port Scanning attack'\n",
    "test = CLLMSample(attack)\n",
    "test.Create_Embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6894a87-cef3-4580-baac-7f341fb45704",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747217e8-8532-4dbc-8bb4-e5dff8a6244f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db40f28c-ef17-4d23-957c-2f695ac444f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
